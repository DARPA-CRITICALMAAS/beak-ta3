{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information\n",
    "This notebook is about to give you some **examples** about how to use the notebooks and associated functionalities in the **beak** package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a list of raster files based on a pre-defined model definition.\n",
    "Case:\n",
    "- You have a list of raster files in a folder or some subfolders.\n",
    "- You have a model definition for a specific model, stored in the respective model module.\n",
    "- You need a list of files corresponding to the evidence layers defined in the model definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model definition:\n",
      "-----------------\n",
      "Geology_Lithology_Majority\n",
      "Geology_Lithology_Minority\n",
      "Geology_Period_Maximum_Majority\n",
      "Geology_Period_Minimum_Majority\n",
      "Geology_Dictionary_Calcareous\n",
      "Geology_Dictionary_Carbonaceous\n",
      "Geology_Dictionary_FineClastic\n",
      "Geology_Dictionary_Felsic\n",
      "Geology_Dictionary_Intermediate\n",
      "Geology_Dictionary_UltramaficMafic\n",
      "Geology_Dictionary_Anatectic\n",
      "Geology_Dictionary_Gneissose\n",
      "Geology_Dictionary_Schistose\n",
      "Terrane_Proximity\n",
      "Geology_PassiveMargin_Proximity\n",
      "Geology_BlackShale_Proximity\n",
      "Geology_Fault_Proximity\n",
      "Geology_Paleolatitude_Period_Maximum\n",
      "Geology_Paleolatitude_Period_Minimum\n",
      "Gravity_GOCE_ShapeIndex\n",
      "Gravity_Bouguer\n",
      "Gravity_Bouguer_HGM\n",
      "Gravity_Bouguer_UpCont30km_HGM\n",
      "Gravity_Bouguer_HGM_Worms_Proximity\n",
      "Gravity_Bouguer_UpCont30km_HGM_Worms_Proximity\n",
      "Magnetic_HGM\n",
      "Magnetic_LongWavelength_HGM\n",
      "Magnetic_HGM_Worms_Proximity\n",
      "Magnetic_LongWavelength_HGM_Worms_Proximity\n",
      "Seismic_LAB_Hoggard\n",
      "Seismic_Moho\n"
     ]
    }
   ],
   "source": [
    "from beak.models import mvt_nat\n",
    "\n",
    "MODEL = \"MVT_PREFERRED\"\n",
    "model = mvt_nat.models[MODEL]\n",
    "\n",
    "print(\"Model definition:\")\n",
    "print(\"-----------------\")\n",
    "for layer, value in model.items():\n",
    "  if value is True: \n",
    "    print(layer)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load file paths: You can use the practical handling from the package, if data are stored in the beak data folder.<br>\n",
    "Also, you can provide multiple folders to load from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib_resources import files\n",
    "\n",
    "BASE_PATH = files(\"beak.data\") / \"LAWLEY22-EXPORT\" / \"EPSG_4326_RES_0_05\" / \"COMPLETE_DATASET\"\n",
    "\n",
    "PATH_NUMERICAL = BASE_PATH / \"NUMERICAL_MINMAX\"\n",
    "PATH_CATEGORICAL = BASE_PATH / \"CATEGORICAL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Sequence, Union\n",
    "from collections import Counter\n",
    "\n",
    "def load_model(\n",
    "    model: dict,\n",
    "    folders: Sequence[Path],\n",
    "    file_extensions: Sequence[str] = [\".tif\", \".tiff\"],\n",
    "    exclude_files: Sequence[Union[Path, str]] = [],\n",
    "    verbose: int = 1,\n",
    "):\n",
    "    # Load evidence layers from model dictionary\n",
    "    print(\"Loading model definition...\")\n",
    "    evidence_layers = []\n",
    "    for layer, value in model.items():\n",
    "        if value == True:\n",
    "            evidence_layers.append(layer)\n",
    "\n",
    "    if not evidence_layers:\n",
    "        raise ValueError(\"No valid selection.\")\n",
    "    else:\n",
    "        print(f\"Selected {str(len(evidence_layers))} evidence layers.\")\n",
    "        if verbose == 1:\n",
    "            [print(f\"- {layer}\") for layer in evidence_layers]\n",
    "            \n",
    "    # Create potential filenames based on evidence layers and file extensions\n",
    "    evidence_layer_files = []\n",
    "    for extension in file_extensions:\n",
    "        for evidence_layer in evidence_layers:\n",
    "            evidence_layer_files.append(evidence_layer + extension)\n",
    "        \n",
    "    # Create file list from provided folders\n",
    "    print(\"\\nCreate file list...\")\n",
    "    file_list = []\n",
    "    \n",
    "    for folder in folders:\n",
    "        for file in folder.rglob(\"*\"):\n",
    "            file = Path(file)\n",
    "\n",
    "            if any(file.suffix.lower() == ext for ext in file_extensions):\n",
    "                file_list.append(file)\n",
    "\n",
    "    if not file_list:\n",
    "        raise ValueError(\"No files found.\")\n",
    "    else:\n",
    "        print(f\"Found {str(len(file_list))} files.\")\n",
    "\n",
    "    # Check if files exist\n",
    "    print(\"\\nSearching for corresponding files...\")\n",
    "    matching_list = []\n",
    "    layers_list = []\n",
    "    for file in file_list:\n",
    "        file_name_lower = file.name.lower()\n",
    "        for layer_file in evidence_layer_files:\n",
    "            layer_file_lower = layer_file.lower()\n",
    "            if file_name_lower == layer_file_lower:\n",
    "                matching_list.append(file)\n",
    "                layers_list.append(Path(layer_file).stem)\n",
    "        \n",
    "    file_list = matching_list\n",
    "    \n",
    "    if not file_list:\n",
    "        raise ValueError(\"No matching files found.\")\n",
    "    else:\n",
    "        print(f\"Found {str(len(file_list))} matching files.\")\n",
    "        if verbose == 1:\n",
    "            [print(f\"- {file}\") for file in file_list]\n",
    "\n",
    "    # Check if all layers have files\n",
    "    print(\"\\nEnsuring that all layers have matching files...\")\n",
    "    missing_layers = []\n",
    "    for layer in evidence_layers:\n",
    "        if not any(layer in file.stem for file in file_list):\n",
    "            missing_layers.append(layer)\n",
    "    \n",
    "    if missing_layers:\n",
    "        [print(f\"ERROR: No file found for evidence layer '{layer}'.\") for layer in missing_layers]\n",
    "        raise ValueError(\"\\nMissing files. Exit.\")\n",
    "    else:\n",
    "        print(\"All layers have matching files.\")\n",
    "        \n",
    "    # Count the occurrences of each filename\n",
    "    print(\"\\nChecking files for multiple occurences...\")\n",
    "    filename_counts = Counter([file.name for file in file_list])\n",
    "\n",
    "    # Print the filenames that occur multiple times and their counts\n",
    "    if max(filename_counts.values()) == 1:\n",
    "        print(\"No duplicates found. All filenames occur only once.\")\n",
    "    else:\n",
    "        if verbose == 1:    \n",
    "            for filename, count in filename_counts.items():\n",
    "                if count > 1:\n",
    "                    print(f\"- '{filename}' occurs {count} times\")\n",
    "        else:\n",
    "            print(f\"Some filenames occur multiple times. Please check with option verbose=1 to see which files are affected.\")\n",
    "\n",
    "    # Exclude files\n",
    "    if exclude_files:\n",
    "        print(\"\\nExcluding files from provided list...\")\n",
    "        for i, file in enumerate(file_list):\n",
    "            if str(file) in exclude_files:\n",
    "                print(f\"- {file}\")\n",
    "                file = Path(file)                \n",
    "                file_list.remove(file)\n",
    "                layers_list.remove(layers_list[i])\n",
    "        \n",
    "    if len(evidence_layers) != len(file_list):\n",
    "        print(f\"\\nWARNING: Number of evidence layers ({str(len(evidence_layers))}) does not match number of files found ({str(len(file_list))}).\")\n",
    "        print(f\"Can be ignored if the model contains multiple files per layer, e.g. binary encoded categoricals.\")\n",
    "    if len(layers_list) != len(file_list):\n",
    "        raise ValueError(\"Number of layers and does not match number of files found. Please check manually excluded files and file extensions.\")\n",
    "    \n",
    "    layers_files = zip(layers_list, file_list)\n",
    "\n",
    "    return evidence_layers, layers_files, filename_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model definition...\n",
      "Selected 31 evidence layers.\n",
      "\n",
      "Create file list...\n",
      "Found 812 files.\n",
      "\n",
      "Searching for corresponding files...\n",
      "Found 18 matching files.\n",
      "\n",
      "Ensuring that all layers have matching files...\n",
      "ERROR: No file found for evidence layer 'Geology_Lithology_Majority'.\n",
      "ERROR: No file found for evidence layer 'Geology_Lithology_Minority'.\n",
      "ERROR: No file found for evidence layer 'Geology_Period_Maximum_Majority'.\n",
      "ERROR: No file found for evidence layer 'Geology_Period_Minimum_Majority'.\n",
      "ERROR: No file found for evidence layer 'Geology_Dictionary_Calcareous'.\n",
      "ERROR: No file found for evidence layer 'Geology_Dictionary_Carbonaceous'.\n",
      "ERROR: No file found for evidence layer 'Geology_Dictionary_FineClastic'.\n",
      "ERROR: No file found for evidence layer 'Geology_Dictionary_Felsic'.\n",
      "ERROR: No file found for evidence layer 'Geology_Dictionary_Intermediate'.\n",
      "ERROR: No file found for evidence layer 'Geology_Dictionary_UltramaficMafic'.\n",
      "ERROR: No file found for evidence layer 'Geology_Dictionary_Anatectic'.\n",
      "ERROR: No file found for evidence layer 'Geology_Dictionary_Gneissose'.\n",
      "ERROR: No file found for evidence layer 'Geology_Dictionary_Schistose'.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nMissing files. Exit.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from beak.utilities.io import load_model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m layers, matches, counts \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mfolders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mPATH_NUMERICAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH_CATEGORICAL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mfile_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.tif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.tiff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mexclude_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 78\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model, folders, file_extensions, exclude_files, verbose)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_layers:\n\u001b[0;32m     77\u001b[0m     [\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: No file found for evidence layer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m missing_layers]\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMissing files. Exit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll layers have matching files.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: \nMissing files. Exit."
     ]
    }
   ],
   "source": [
    "# from beak.utilities.io import load_model\n",
    "\n",
    "layers, matches, counts = load_model(model=model, \n",
    "                                     folders=[PATH_NUMERICAL, PATH_CATEGORICAL], \n",
    "                                     file_extensions=[\".tif\", \".tiff\"], \n",
    "                                     exclude_files=[], \n",
    "                                     verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_load_models():\n",
    "    folder = Path(\n",
    "        \"paste_path_here\"\n",
    "    )\n",
    "    \n",
    "    model = { \"dummy\": False, \"utils\": True, \"eda\": True, \"nat\": True, \"rolling_stone\": False }\n",
    "    file_extensions = [\".py\", \".txt\"]\n",
    "    exclude_files = [\"paste_excluded_files_here\"]\n",
    "    \n",
    "    layers, matches, counts = load_model(model,\n",
    "                                        folder,\n",
    "                                        file_extensions,\n",
    "                                        exclude_files,\n",
    "                                        )\n",
    "    \n",
    "    layers_matched, files_matched = zip(*list(matches))\n",
    " \n",
    "    print(f\"\\nLayers: {layers}:\")\n",
    "    for i, layer in enumerate(layers_matched):\n",
    "        if i < len(files_matched):\n",
    "            print(f\"- {layer}: {files_matched[i]}\")\n",
    "            \n",
    "# endregion: Test code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beak-ta3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
